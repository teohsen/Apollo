{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qzlZajz-TwS-"
      },
      "outputs": [],
      "source": [
        "# RNN Intuition\n",
        "\n",
        "my_rnn = RNN()  # Some RNN\n",
        "hidden_state = [0,0,0,0]\n",
        "\n",
        "sentence = [\"I\", \"love\", \"recurrent\", \"neural\"]\n",
        "\n",
        "for word in sentence:\n",
        "    prediction, hidden_state = my_rnn(word, hidden_state)\n",
        "\n",
        "next_word_prediction = prediction\n",
        "# >>> \"networks!\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "RnZxU098VRLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyRNNCell(tf.keras.layers.Layer):\n",
        "  def __init__(self, rnn_units, input_dim, output_dim):\n",
        "    super(MyRNNCell, self).__init()\n",
        "\n",
        "    # Initalize weight matrics\n",
        "    self.W_xh = self.add_weights([rnn_units, input_dim]) # Input\n",
        "    self.W_hh = self.add_weights([rnn_units, rnn_units]) # temporal matrics\n",
        "    self.W_hy = self.add_weights([output_dim, rnn_units]) # Output\n",
        "\n",
        "    # Initalize hidden state to zeros\n",
        "    self.h = tf.zeros([rnn_units, 1])\n",
        "\n",
        "  def call(self, x):\n",
        "    # Update the hidden state\n",
        "    self.h = tf.math.tanh(self.W_hh * self.h + self.W_xh * x)\n",
        "\n",
        "    # compute the output\n",
        "    output = self.W_hy * self.h\n",
        "\n",
        "    # return the current output and hidden state\n",
        "    return output, self.h"
      ],
      "metadata": {
        "id": "s2e4eIYyT6Oc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# or just use the tensorflow implementation of the above\n",
        "\n",
        "tf.keras.layers.SimpleRNN(rnn_units)"
      ],
      "metadata": {
        "id": "ZeKnL3KWU2Ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xr1wbnL2VDjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNNs for Sequence Modeling\n",
        "\n",
        "Design Criteria\n",
        "\n",
        "To model sequences, we need to:\n",
        "1. Handle variable-length sequences\n",
        "2. Track long-term dependencies\n",
        "3. Maintain information about order\n",
        "4. Share parameters across the sequences (the weight needs to be able to share across different time steps and still produce meaningful predictions)\n",
        "\n",
        "\n",
        "\n",
        "# Modeling problem\n",
        "\"Predict the next word\"\n",
        "\n",
        "e.g \"This morning I took my cat for a **walk**.\"\n",
        "\n",
        "Goal is to predict the word \"walk\"\n",
        "\n",
        "- How, representing Language to a Neural Network\n",
        "- Encoding Language for a Neural Network\n",
        "\n",
        "Concept: Embedding\n",
        "\n",
        "Transform indexes into a vector of fixed size.\n",
        "\n",
        "1. Vocabulary: Corpus of words\n",
        "2. Indexing: word to index\n",
        "      e.g a --> 1\n",
        "          cat --> 2\n",
        "\n",
        "3. Embedding: index to fixed-sized vector\n",
        "e.g One-hot Embedding\n",
        "\"cat\" = [ 0 ,1 ,0,0,0,0,0[\n",
        "e.g. Learned Embedding (neural network for embedding)\n",
        "\n",
        "---\n",
        "Backpropagation Through Time (BPTT)\n",
        "\n",
        "\n",
        "Recall: Backpropagation in Feed Forward models\n",
        "Backpropagation algorithm\n",
        "1. Take the derivative (gradient) of the loss w.r.t to each parameter\n",
        "2. Shift parameters in order to minimize loss\n",
        "\n",
        "\n",
        "With RNN w regards to Temporal unrolling\n",
        "- Backward pass through model state\n",
        "\n",
        "Standard RNN Gradient Flow\n",
        "- Computing the gradient wrt h0 involves many factors of W_hh + repeated gradient computation!\n",
        "\n",
        "Issues:\n",
        "\n",
        "Weight Matrics\n",
        "- Many Values >1; Exploding Gradients\n",
        "  - Mitigating: Gradient Clipping to scale big gradients\n",
        "- Many Values <1: Vanishing gradients\n",
        "  [Mitgating methods]\n",
        "  - Activation function\n",
        "  - Weight initalization\n",
        "  - Network Architecture\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "The Problem of long-term dependencies\n",
        "\n",
        "Why are vanishing gradients a problem?\n",
        "Multiply many small numbers together --> Errors due to further back time steps have smaller and smaller gradients --> Bias parameters to capture short-term dependencies\n",
        "\n",
        "RNN becomes unable to predict when the required \"info\" is very far apart.\n",
        "\n",
        "- Tricks #1: Activation Functions\n",
        "  Using ReLU prevents f' from shrinking the gradients when x > 0\n",
        "- Trick #2: Parameter Initialization\n",
        "  Initalize weights to identity matrix, ini\n",
        "- Trick #3: Gated Cells (Most robust to handle long term dependencies)\n",
        "  idea: Use Gates to selectively add or remove information within each recurrent unit with\n",
        "  - LSTM networks rely on a gated cell to track informaton throughout\n",
        "\n",
        "  Gated LSTM Key Concepts\n",
        "  - Maintaining a cell states (like a standard rnn)\n",
        "  - Use gates to control the flow of information\n",
        "    - Forget gate gets rid of irrelevant information\n",
        "    - Store relevant information from current input\n",
        "    - Selectively update cell state\n",
        "    - Ouput gate returns a filtered version of the cell state\n",
        "  - Backpropagation through time with partially uniterrupted gradient flow\n",
        "\n",
        "---\n",
        "RNN Applications & Limitations\n",
        "\n",
        "e.g Music Generation\n",
        "Input: Sheet music\n",
        "Output: Next character in sheet music\n",
        "\n",
        "e.g Sentiment Classification\n",
        "Sequence of input text to single output\n",
        "- Tweet sentiment classification\n",
        "\n",
        "\n",
        "\n",
        "[Limitations of RNNs]\n",
        "1. Encoding Bottleneck\n",
        "  - how to encode till the end of time without much loss\n",
        "2. Slow, no parllelization\n",
        "3. Not long memory\n",
        "  - capacity of RNN, LSTM is not that long (handling like millions of words)\n",
        "\n",
        "\n",
        "[Desired Capabilities]\n",
        "- Continuous stream\n",
        "- Parallelization\n",
        "- Long memory\n",
        "\n",
        "---\n",
        "Hence leading to\n",
        "\n",
        "**Attention is All You Need**\n",
        "  - Foundation of the Transformer Architecture\n",
        "  - Intuition behind self-attention\n",
        "    - Attending to the most important parts of an input\n",
        "      1. Identifty which parts to attend to (Similar to a search problem!)\n",
        "        - Understanding Attention with Search\n",
        "        - Find Overlaps between **Query** and **Key**, how similar?\n",
        "          - Compute attention mask\n",
        "      2. Extract the features with high attention\n",
        "        \n",
        "\n",
        "Learning self-attention with Neural networks\n",
        "\n",
        "e.g. \"He tossed the tennis ball to serve\"\n",
        "1. Encode position information\n",
        "  - Position-aware encoding\n",
        "  - Data is fed in all at once! need to encode position information to understand order\n",
        "2. Extract query, key, value for search\n",
        "  - Positional Embedding * Linear Layer for Q, K, V\n",
        "3. Compute attention weighting\n",
        "  - Attention score: computer pairwise similarity between each query and Key\n",
        "  - How to compute similarity between 2 sets of features?\n",
        "    - Usually dot product (a.k.a cosine similarity)\n",
        "4. Extract features with high attention\n",
        "  - Attention Matrix * Value = Output\n",
        "\n",
        "\n",
        "Goal: Identify and attend to most important features in input\n",
        "\n",
        "softmax( Q*Kt / Scaling ) * V\n",
        "\n",
        "These operations form a self-attention head that can plug into a larger network.\n",
        "each head attends to a different part of input\n",
        "\n",
        "--> forms a rich network\n",
        "\n",
        "\n",
        "**Self-attention applied**\n",
        "1. Language Processing\n",
        "  - BERT, GPT3 (and more)\n",
        "2. Biological sequences\n",
        "  - AlphaFold2\n",
        "3. Computer vision\n",
        "  - Vision Transformers\n",
        "\n",
        "\n",
        "---\n",
        "**Summary**\n",
        "1. RNNs are well suited for sequence modeling tasks\n",
        "2. Model sequences via a recurrence relation\n",
        "3. Training RNNs with backpropagation through time\n",
        "4. Modeling for e.g music\n",
        "5. Self-attention to model sequences without recurrence"
      ],
      "metadata": {
        "id": "mS4MNakhVi9B"
      }
    }
  ]
}